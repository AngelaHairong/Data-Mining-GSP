# -*- coding: utf-8 -*-
"""RedditScrapping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w3SGYT0bN5Q6xGBU4ywQha6l3rOKVAxf
"""

pip install praw

import praw
import csv

def get_reddit_posts(subreddit_name, num_posts=1500):
    # Set up your Reddit API credentials
    reddit = praw.Reddit(client_id='oyHnXvOG3guQNAB4oqaCow',
                         client_secret='K67rTtFUxzI-bEz4s1Fe4TuCLIHwEg',
                         user_agent='GSP project/1.0 by Suspicious-Fruit7117')

    # Choose the subreddit
    subreddit = reddit.subreddit(subreddit_name)

    # Initialize an empty list to store post titles
    post_titles = []

    # Set the number of posts to retrieve per request (max is 100)
    posts_per_request = 100

    # Calculate the number of requests needed
    num_requests = (num_posts + posts_per_request - 1) // posts_per_request

    # Loop through the pages of posts
    for i in range(num_requests):
        # Get the top posts for the current page
        posts = subreddit.top(limit=posts_per_request, params={'after': post_titles[-1] if post_titles else None})

        # Append the titles to the list
        post_titles.extend([post.title for post in posts])

    return post_titles[:num_posts]

def save_to_csv(posts, filename):
    with open(filename, 'w', encoding='utf-8', newline='') as file:
        writer = csv.writer(file)
        writer.writerows([[post] for post in posts])

if __name__ == "__main__":
    subreddit_name = "your_subreddit_name"
    num_posts = 1500
    output_csv_file = 'AdvancedIdeas'

    # Get Reddit posts
    reddit_posts = get_reddit_posts(subreddit_name, num_posts)

    # Save to CSV
    save_to_csv(reddit_posts, output_csv_file)

    print(f"{num_posts} Reddit posts saved to {output_csv_file}")