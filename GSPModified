from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
import pandas as pd
from itertools import combinations

# Download necessary NLTK resources
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)

# Load Reddit data
reddit_data = pd.read_csv("reddit_posts.csv")
reddit_data.columns = ['Post']
print(reddit_data.head())

# Text transformation function
def transform_text_to_sequences(dataframe, column_name):
    stop_words = set(stopwords.words('english'))
    return [
        [word.lower() for word in word_tokenize(post) if word.isalpha() and word.lower() not in stop_words]
        for post in dataframe[column_name]
    ]

# Apply transformation
reddit_sequences = transform_text_to_sequences(reddit_data, 'Post')
print(reddit_sequences[:5])

# Candidate generation function
def generate_candidates(items, length):
    items_list = list(items)  # Convert set to list
    return set(
        frozenset(combined_set)
        for i in range(len(items_list))
        for j in range(i + 1, len(items_list))
        if (combined_set := items_list[i] | items_list[j]) and len(combined_set) == length
    )


# Prune step
def prune_candidates(candidates, previous_frequent, length):
    return {
        candidate for candidate in candidates
        if all(frozenset(comb) in previous_frequent for comb in combinations(candidate, length - 1))
    }

# Support counting function
def count_support(sequences, candidates):
    support_counts = {candidate: 0 for candidate in candidates}
    for sequence in sequences:
        sequence_set = set(sequence)
        for candidate in candidates:
            if candidate.issubset(sequence_set):
                support_counts[candidate] += 1
    return support_counts

# GSP algorithm
def gsp_algorithm(sequences, min_support):
    length = 1
    frequent_sequences = set()
    current_frequent = {frozenset([item]) for sequence in sequences for item in sequence if sequence.count(item) >= min_support}

    while current_frequent:
        print(f"Length: {length}, Current Frequent Items: {current_frequent}")
        candidates = generate_candidates(current_frequent, length + 1)
        print(f"Candidates: {candidates}")

        if not candidates:
            break

        candidates = prune_candidates(candidates, current_frequent, length + 1)
        support_counts = count_support(sequences, candidates)
        current_frequent = {candidate for candidate, count in support_counts.items() if count >= min_support}
        frequent_sequences.update(current_frequent)
        length += 1

    return frequent_sequences

# Run the algorithm
min_support = 2
frequent_items = gsp_algorithm(reddit_sequences, min_support)
print(f"Frequent Items: {frequent_items}")
