# -*- coding: utf-8 -*-
"""TextTrasnformation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GBhuluyHsLhYuxvlK6asIYezas4YyPiI
"""

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
import pandas as pd
# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
reddit_data = pd.read_csv("/content/reddit_posts.csv")
reddit_data.columns = ['Post']
print(reddit_data.head())
# Define a function for text transformation
def transform_text_to_sequences(dataframe, column_name):
    stop_words = set(stopwords.words('english'))
    sequences = []

    for post in reddit_data['Post']:
        # Tokenize the post
        tokens = word_tokenize(post)
        # Remove stopwords and non-alphabetic words
        filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word not in stop_words]
        sequences.append(filtered_tokens)

    return sequences

# Apply the transformation to the Reddit dataset
reddit_sequences = transform_text_to_sequences(reddit_data, 1)

# Display the first few transformed sequences
reddit_sequences[:5]

# Efficient candidate generation with join and prune steps
def join_step(frequent_items, length):
    candidates = set()
    items = list(frequent_items)
    for i in range(len(items)):
        for j in range(i + 1, len(items)):
            combined_set = items[i].union(items[j])
            if len(combined_set) == length:
                candidates.add(frozenset(combined_set))
    return candidates



def prune_step(candidates, previous_frequent, length):
    return {candidate for candidate in candidates if all(frozenset(combination) in previous_frequent for combination in combinations(candidate, length - 1))}

# Adjusted support counting function
def count_support(sequences, candidates):
    support_counts = {candidate: 0 for candidate in candidates}
    for sequence in sequences:
        sequence_set = set(sequence)
        for candidate in candidates:
            if candidate.issubset(sequence_set):
                support_counts[candidate] += 1
    return support_counts

# Lower the minimum support for testing
min_support = 2

# Add debugging statements in the GSP algorithm
def gsp_algorithm(sequences, min_support):
    length = 2
    frequent_sequences = set()
    current_frequent_items = {frozenset([item]) for sequence in sequences for item in sequence}

    while True:
        print(f"Length: {length}, Current Frequent Items: {current_frequent_items}")
        candidates = join_step(current_frequent_items, length)
        print(f"Candidates: {candidates}")
        candidates = prune_step(candidates, current_frequent_items, length)

        support_counts = count_support(sequences, candidates)
        current_frequent = {candidate for candidate, count in support_counts.items() if count >= min_support}

        if not current_frequent:
            break

        frequent_sequences.update(current_frequent)
        current_frequent_items = current_frequent
        length += 1

    return frequent_sequences

# Run the algorithm
frequent_items = gsp_algorithm(reddit_sequences, min_support)
print(f"Frequent Items: {frequent_items}")