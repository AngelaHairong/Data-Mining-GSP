# -*- coding: utf-8 -*-

import matplotlib.pyplot as plt
import networkx as nx
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
import pandas as pd
from itertools import combinations, product

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Load Reddit data
reddit_data = pd.read_csv("reddit_posts.csv")
reddit_data.columns = ['Post']
print(reddit_data.head())

# Function to transform text to sequences
def transform_text_to_sequences(dataframe, column_name):
    stop_words = set(stopwords.words('english'))
    sequences = []

    for post in dataframe[column_name]:
        # Tokenize the post
        tokens = word_tokenize(post)
        # Remove stopwords and non-alphabetic words
        filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word not in stop_words]
        sequences.append(filtered_tokens)

    return sequences

# Apply the transformation to the Reddit dataset
reddit_sequences = transform_text_to_sequences(reddit_data, 'Post')

# Function to check if candidate is within max gap in a sequence
def is_within_max_gap(sequence, candidate, max_gap):
    indices = [sequence.index(item) for item in candidate if item in sequence]
    return all(abs(i - j) <= max_gap for i, j in combinations(indices, 2))

# Function to count support considering the max gap
def count_support_with_gap(sequences, candidates, max_gap):
    support_counts = {candidate: 0 for candidate in candidates}
    for sequence in sequences:
        for candidate in candidates:
            if candidate.issubset(sequence) and is_within_max_gap(sequence, candidate, max_gap):
                support_counts[candidate] += 1
    return support_counts

# Efficient candidate generation with join and prune steps
def join_step(frequent_items, length):
    candidates = set()
    items = list(frequent_items)
    for i in range(len(items)):
        for j in range(i + 1, len(items)):
            combined_set = items[i].union(items[j])
            if len(combined_set) == length:
                candidates.add(frozenset(combined_set))
    return candidates

def prune_step(candidates, previous_frequent, length):
    return {candidate for candidate in candidates if all(frozenset(combination) in previous_frequent for combination in combinations(candidate, length - 1))}

# GSP algorithm with maxGap and short rules
def gsp_algorithm(sequences, min_support, max_gap, max_length):
    length = 2
    frequent_sequences = set()
    current_frequent_items = {frozenset([item]) for sequence in sequences for item in sequence}

    while length <= max_length:
        print(f"Length: {length}, Current Frequent Items: {current_frequent_items}")
        candidates = join_step(current_frequent_items, length)
        print(f"Candidates: {candidates}")
        candidates = prune_step(candidates, current_frequent_items, length)

        support_counts = count_support_with_gap(sequences, candidates, max_gap)
        current_frequent = {candidate for candidate, count in support_counts.items() if count >= min_support}

        if not current_frequent:
            break

        frequent_sequences.update(current_frequent)
        current_frequent_items = current_frequent
        length += 1

    return frequent_sequences

def visualize_frequent_items(frequent_items):
    # Create a graph
    G = nx.Graph()
    
    # Add nodes and edges from frequent item sets
    for item_set in frequent_items:
        item_list = list(item_set)
        G.add_nodes_from(item_list)
        G.add_edges_from(combinations(item_list, 2))

    # Calculate node size based on degree to visualize importance
    degrees = dict(G.degree())
    node_size = [v * 100 for v in degrees.values()]  # Scale the node sizes
    
    # Choose a layout that spaces out the nodes
    pos = nx.spring_layout(G, k=0.15, iterations=20)
    
    # Draw the graph with a larger figure size
    plt.figure(figsize=(15, 15))
    nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color='skyblue', alpha=0.7)
    nx.draw_networkx_edges(G, pos, alpha=0.2)  # Use transparency for edges
    nx.draw_networkx_labels(G, pos, font_size=8)  # Adjust font size accordingly

    # Set title and show the graph
    plt.title("Frequent Item Sets", fontsize=16)
    plt.axis('off')  # Turn off the axis
    plt.show()



# Example usage of the algorithm
min_support = 10
max_gap = 3
max_length = 3  # For short rules
frequent_items = gsp_algorithm(reddit_sequences, min_support, max_gap, max_length)
visualize_frequent_items(frequent_items)
print(f"Frequent Items: {frequent_items}")
